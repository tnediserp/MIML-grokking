anneal_lr: false
anneal_lr_steps: 500000
batchsize: 0
checkpoint_path: /root/checkpoints
ckpt_epoch: '0'
d_key: 32.0
d_model: 128
datadir: /root/data
dropout: 0.1
gpu: 0
logdir: /root
math_operator: +
max_context_len: 50
max_epochs: null
max_lr: 0.001
max_steps: 500000
model: Transformer
n_heads: 4
n_layers: 2
noise_factor: 0
non_linearity: relu
operand_length: null
random_seed: -1
save_activations: false
save_outputs: false
train_data_pct: 15
warmup_steps: 10
weight_decay: 0.1
weight_decay_kind: to_zero
weight_noise: 0.0

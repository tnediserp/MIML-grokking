\begin{abstract}
    % Grokking, a phenomenon that the generalization of a neural network happens much later than the convergence of its training loss, has received increasing attention from both learning theory and application since it was first introduced by [Power et al., 2022].
    We reproduce the grokking phenomenon [Power et al., 2022], that a neural network generalizes long after it memorizes the training data, for modular addition problem, and provide an explanation based on [Kumar et al., ICLR 2024].
    \footnote{All the codes and supplementary materials are available at: \url{https://github.com/tnediserp/MIML-grokking}}
\end{abstract}
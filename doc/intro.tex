\section{Introduction}
\label{sec:intro}

In the field of machine learning, it is an important goal to understand the generalization dynamics of neural networks.
Typically, the model's generalization ability can be well reflected by its performance on the training data.
However, when training neural networks for certain problems, generalization may occur long after the model overfits the training data. 
This striking phenomenon is called \emph{grokking}~\cite{Grokking}. 
Ever since it was first introduced, exhausted efforts have been made to understand grokking from a theoretical viewpoint.
For example, grokking is explained by the hardness of representation learning~\cite{LiuKNMTW22},
the effect of weight decay and weight norm decrease~\cite{Grokking_circuit_efficiency,LiuMT23}
and the transition between different training regimes~\cite{KumarBGP24,MohamadiLWS24}.

Among those learning tasks where grokking can be observed, the most well-studied one might be the \emph{modular addition problem}~\cite{Grokking,KumarBGP24,MohamadiLWS24,Gromov}.
Specifically, given a fixed prime number $p$, consider learning the output of the modular arithmetic: % where each equation consists of $K$ summands and is represented as a string:
\begin{equation}
    a_{1} + a_{2} + \cdots + a_{K} = b,
    \label{eqn:modular_addition}
\end{equation}
where both the individual summands $a_i$ and the sum $b$ are elements of the finite field $\mathbb{Z}/p\mathbb{Z}$.
We model \eqref{eqn:modular_addition} as a classification problem, where classes are labeled by integers $b \in \{0, 1, \dots, p-1\}$.
We mainly focus on the case $K=2$, and discuss general $K$-wise addition in \cref{sec:subtask4}.

% Our experiments are divided into 4 parts in section 3. \ref{sec:subtask1} reproduces the grokking phenomenon stated in \cite{Grokking} and study the effect of $\alpha$ on grokking phenomenon. \ref{sec:subtask2} uses other models such as LSTM and MLP to conduct the same task. \ref{sec:subtask3} investigates the impact of different optimizers(with different hyper-Parameters such as learning rate, weight decay and dropout) to grokking phenomenon. \ref{sec:subtask4} explores the grokking phenomenon for different $K$.  

% At the end of our report we give some explanations on grokking phenomenon, mainly based on \cite{JacotHG18}.
